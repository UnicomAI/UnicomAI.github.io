<!DOCTYPE html>
<html>
<meta property='og:title' content="LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation"/>
<meta property='og:description' content="LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation"/>
<meta property='og:url' content='https://unicomai.github.io/LeMiCa/'/>
<meta property='og:image:width' content='1200' />
<meta property='og:image:height' content='663' />
<meta property="og:type" content='website'/>
<head>
  <meta charset="utf-8">
  <meta name="description" content="LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation">
  <meta name="keywords" content="LeMiCa, Diffusion Video Generation, Caching, Lexicographic Minimax">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment</title>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/tab_gallery.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/magnifier.js"></script>
  <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/image_card_fader.css">
  <link rel="stylesheet" href="./static/css/image_card_slider.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Patrick+Hand|Google+Sans|Noto+Sans|Castoro|Lato|Open+Sans&effect=shadow-multiple|emboss|3d"> 
  <link rel="icon" href="./static/images/logo.png" type="image/x-icon"> 
  <link rel="shortcut icon" href="./static/images/logo.png" type="image/x-icon">
  <script>
    function copyToClipboard() {
      var code = document.getElementById("code-snippet").innerText;
      navigator.clipboard.writeText(code).then(function() {
        alert('Code copied to clipboard!');
      }, function(err) {
        alert('Failed to copy text: ', err);
      });
    }
</script>
<script>
    var img = document.getElementById("imageToFullscreen");
    img.addEventListener("click", () => {
      if (!document.fullscreenElement) {
        // 进入全屏
        img.requestFullscreen().catch(err => {
          console.error(`无法进入全屏模式: ${err.message}`);
        });
      } else {
        // 退出全屏
        document.exitFullscreen();
      }
    });
  </script>

  <!--  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> -->
  <script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  };
  </script>
  <script id="MathJax-script" async 
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>

<body>
  <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <br/>
          <br/>
          <h1 class="title is-1 publication-title" style="font-size: 2.12rem">
                       <img src="./static/images/logo.png" style="height:50px; margin-bottom: -10px;"></img>
                       <span class="lemica">HiMo-CLIP</span>
          </h1>
          <h2 class="title is-2 publication-title" style="margin-bottom: 40px"><strong>Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment</strong></h2>
          
          <p class="subtitle is-4" style="margin-top: 0.5rem;">✨ AAAI 2026 Oral ✨</p>
    
          <div class="is-size-4 publication-authors" style="font-size: 2rem; font-weight: bold;">
            <span class="author-block">
              <a href="" target="_blank">Ruijia Wu</a><sup>1,2</sup><sup>†</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&view_op=list_works&user=gpNOW2UAAAAJ" target="_blank">Ping Chen</a><sup>1,2</sup><sup>†</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=wqvr28MAAAAJ&hl=zh-TW" target="_blank">Fei Shen</a><sup>3</sup>,&nbsp;
            </span>
            <span class="author-block">
              Shaoan Zhao<sup>1,2</sup>,&nbsp;
            </span>
            <span class="author-block">
               Qiang Hui<sup>1,2</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://github.com/joelulu" target="_blank">Huanlin Gao</a><sup>1,2</sup>,&nbsp;
            </span>
            <span class="author-block">
              Ting Lu<sup>1,2</sup>,&nbsp;
            </span>
            <span class="author-block">
               <a href="https://scholar.google.com/citations?hl=en&user=L4OXOs0AAAAJ" target="_blank">Zhaoxiang Liu</a><sup>1,2</sup>
            </span>
            <span class="author-block">
              <a href="https://github.com/FangGet" target="_blank">Fang Zhao</a><sup>1,2</sup><sup>*</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=CFUQLCAAAAAJ&hl=en" target="_blank">Kai Wang</a><sup>1,2</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=kCC2oKwAAAAJ&hl=zh-CN&oi=ao" target="_blank">Shiguo Lian</a><sup>1,2</sup><sup>*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Data Science & Artificial Intelligence Research Institute, China Unicom,&nbsp;</span>
            <span class="author-block"><sup>2</sup>Unicom Data Intelligence, China Unicom</span>
            <span class="author-block"><sup>3</sup>National University of Singapore</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">(† Equal contribution. * Corresponding author.)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link -->
              <span class="link-block">
                <a href="https://github.com/UnicomAI/HiMo-CLIP" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Paper Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2511.06653" class="external-link button is-normal is-rounded is-dark" target="_blank"> <!-- 替换为你的论文链接 -->
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
              <a class="external-link button is-normal is-rounded is-dark" href="" target="_blank">
              <span class="icon">
                <i class="fas fa-share-square">
                </i>
              </span>
              <span>
                Model
              </span>
              </a>
            </span> -->

            <!-- <span class="link-block">
              <a class="external-link button is-normal is-rounded is-dark" href="" target="_blank">
              <span class="icon">
                <i class="fas fa-database">
                </i>
              </span>
              <span>
                Dataset
              </span>
              </a>
            </span> -->

              <!-- Project Page Link -->
              <span class="link-block">
                <a href="https://unicomai.github.io/HiMo-CLIP" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-globe"></i>
                  </span>
                  <span>Project Page</span>
                </a>
              </span>
              <!-- Bibtex Link -->
              <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-obp"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
    <br>
      <!-- 替换为你的 teaser 图路径 -->
      <div style="text-align: center;">
        <img src="./static/images/motivation.png" alt="HiMo-CLIP Motivation Figure" style="width: 75%; display: block; margin: 0 auto;">
      </div>
      <span style="font-size: 0.8em; width: 100%; display: inline-block;">(a) Text descriptions of an image often grow in semantic richness, from short to long, by adding more visual details. (b) However, existing models, even those tailored for long-form text, often fail to preserve semantic monotonicity, overlooking this essential principle when scaling to richer descriptions. In contrast, HiMo-CLIP maintains alignment consistency across text granularities, effectively addressing this overlooked yet critical challenge. (Note: FineLIP’s similarity exceeds 1 due to its customized test-time scaling.)</span>
    </div>
  </div>
</section>

<!-- Paper Abstract -->
<section class="section hero is-light">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Abstract</h2>
      <div class="content has-text-justified">
        <p>
          Contrastive vision-language models like CLIP have achieved impressive results in image-text retrieval by aligning image and text representations in a shared embedding space. However, these models often treat text as flat sequences, limiting their ability to handle complex, compositional, and long-form descriptions. In particular, they fail to capture two essential properties of language: semantic hierarchy, which reflects the multi-level compositional structure of text, and semantic monotonicity, where richer descriptions should result in stronger alignment with visual content.
          To address these limitations, we propose HiMo-CLIP, a representation-level framework that enhances CLIP-style models without modifying the encoder architecture. HiMo-CLIP introduces two key components: a hierarchical decomposition (HiDe) module that extracts latent semantic components from long-form text via in-batch PCA, enabling flexible, batch-aware alignment across different semantic granularities, and a monotonicity-aware contrastive loss (MoLo) that jointly aligns global and component-level representations, encouraging the model to internalize semantic ordering and alignment strength as a function of textual completeness.
          These components work in concert to produce structured, cognitively-aligned cross-modal representations. Experiments on multiple image-text retrieval benchmarks show that HiMo-CLIP consistently outperforms strong baselines, particularly under long or compositional descriptions.
        </p>
      </div>
    </div>
  </div>
</section>
  <!-- End Abstract -->

<!-- Teaser Image/Video -->
<section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
    <br>
      <!-- 替换为你的 teaser 图路径 -->
      <div style="text-align: center;">
        <img src="./static/images/framework.png" alt="HiMo-CLIP Teaser Figure" style="width: 100%; display: block; margin: 0 auto;">
      </div>
      <span style="font-size: 0.8em; width: 100%; display: inline-block;"><b>HiMo-CLIP Framework</b></span>
    </div>
  </div>
</section>
  <!-- End Teaser -->

  <!-- Method Section -->
<section class="section">
  <div class="container is-max-desktop is-centered has-text-centered">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Contrastive vision-language models such as CLIP have shown remarkable performance in aligning images and text within a shared embedding space. However, they typically treat text as flat token sequences, ignoring the compositional and hierarchical nature of language. This simplification limits their ability to process complex and long-form descriptions, where multiple semantic levels coexist.
          </p>
          <p>
            In particular, current models fail to capture two fundamental linguistic properties:
            <br/>
            <b>(1) Semantic Hierarchy</b> — the multi-level compositional structure of textual meaning, and
            <br/>
            <b>(2) Semantic Monotonicity</b> — the principle that richer or more complete descriptions should correspond to stronger alignment with the visual content.
          </p>
          <p>
            These limitations motivate the design of <b>HiMo-CLIP</b>, which explicitly models both hierarchical and monotonic relationships between vision and language representations while remaining compatible with standard CLIP architectures.
          </p>
        </div>


      <h2 class="title is-4">Methods</h2>
      <div class="content has-text-justified">
        <p>
          To address the above limitations, <b>HiMo-CLIP</b> introduces two lightweight, representation-level modules that can be seamlessly integrated into CLIP-style frameworks without altering the encoders:
        </p>
        <ul>
          <li>
            <b>Hierarchical Decomposition (HiDe):</b>
            HiDe performs in-batch Principal Component Analysis (PCA) on textual embeddings to extract the most discriminative latent semantic components.
            These components dynamically adapt to batch context, revealing the intrinsic semantic hierarchy of each text sample.
            By aligning image representations with both global and component-level embeddings, HiDe enables fine-grained and multi-granular alignment.
          </li>
          <li>
            <b>Monotonicity-aware Contrastive Loss (MoLo):</b>
            MoLo jointly aligns the image with its full-text embedding and its decomposed semantic components.
            This design enforces <i>semantic monotonicity</i> — ensuring that alignment strength increases as the text becomes more complete or informative.
            The loss encourages the model to internalize semantic ordering, leading to structured and cognitively aligned vision-language representations.
          </li>
        </ul>
        <p>
          Both modules operate purely in the representation space, avoiding architectural modifications and additional supervision.
          Together, they allow HiMo-CLIP to efficiently capture hierarchical semantics and monotonic alignment properties, achieving superior performance across both long-text and short-text retrieval benchmarks.
        </p>
      </div>

    </div>
  </div>
</section>
<!-- End Method Section -->

<!-- Experiments Section -->
<section class="section">
<div class="container is-max-desktop is-centered has-text-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Performance</h2>
  </div>
</div>

<section class="section" style="margin-top: -1rem">
      <div class="container is-max-desktop">
       <div class="columns is-centered">
        <div class="column is-full-width">
         <div class="content has-text-justified">
          <p>
              <b>HiMo-CLIP</b> consistently outperforms state-of-the-art methods across all long-text benchmarks. Under the ViT-L/14 backbone, our method achieves 93.0%/93.1%(I2T/T2I) on Urban1k, 82.4%/84.4% ((I2T/T2I)) on Docci, and 62.2%/61.9% (I2T/T2I) on Long-DCI, surpassing the strongest baseline (FineLIP) by pretty margins.
          </p>

          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div class="has-text-centered">
                <img src="./static/images/eval_tb1.png" class="table-image" style="width: 100%; max-width: 700px;" />
              </div>
            </div>
            <div class="column is-half">
              <div class="has-text-centered">
                <img src="./static/images/eval_tb2.png" class="table-image" style="width: 100%; max-width: 700px;" />
              </div>
            </div>
            <div class="column is-half">
              <div class="has-text-centered">
                <img src="./static/images/eval_tb3.png" class="table-image" style="width: 100%; max-width: 700px;" />
              </div>
            </div>
          </div>
          <p>
               Figure.3 visualizes HiMo@5 trends on HiMo-Docci, where HiMo-CLIP consistently maintains monotonic similarity growth, unlike CLIP and Long-CLIP which often exhibit erratic drops, validating our core assumption that richer subtexts should yield stronger alignment. Figure.4 and Figure.5 extend this analysis with concrete examples for HiMo@2, @3, @4, and @7, showing that HiMo-CLIP reliably preserves correct score orderings even under deeper hierarchies. For instance, HiMo-CLIP achieves the highest qualitative HiMo@4 (0.93) and HiMo@7 (0.97), while FineLIP and TULIP exhibit score reversals, and Long-CLIP yields negative Pearson correlations ($-0.94$, $-0.95$). On shallower tasks, HiMo-CLIP maintains correct ordering at all steps, while FineLIP and TULIP show violations in HiMo@2 and HiMo@3, and even FG-CLIP fails on HiMo@3 despite strong quantitative scores. These results highlight the robustness and scalability of our representation-level alignment in modeling hierarchical semantic consistency across varied depths and content.
          </p>
         </div>

          <div style="text-align: center;">
            <img id="teaser" src="./static/images/eval_fig3.png" width="85%"/>
          </div>
          <div style="text-align: center;">
            <img id="teaser" src="./static/images/eval_fig4.png" width="85%"/>
          </div>
          <div style="text-align: center;">
            <img id="teaser" src="./static/images/eval_fig5.png" width="85%"/>
          </div>
          <br>
        </div>
       </div>
      </div>
     </section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre>
    <code id="code-snippet">
      @misc{wu2025himoclipmodelingsemantichierarchy,
            title={HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment}, 
            author={Ruijia Wu and Ping Chen and Fei Shen and Shaoan Zhao and Qiang Hui and Huanlin Gao and Ting Lu and Zhaoxiang Liu and Fang Zhao and Kai Wang and Shiguo Lian},
            year={2025},
            eprint={2511.06653},
            archivePrefix={arXiv},
            primaryClass={cs.CV},
            url={https://arxiv.org/abs/2511.06653}, 
      }
  </code></pre>
  </div>
</section>
